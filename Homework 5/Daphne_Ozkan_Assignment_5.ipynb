{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "UzY5NDgsYmn5"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a Decision Tree from Scratch [35 points]"
      ],
      "metadata": {
        "id": "XgI97LJuZEZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Node Representation [3 points]**\n",
        "\n",
        "- **Purpose**: Represents a node in the decision tree, which can either be a leaf or an internal node.\n",
        "- **Attributes**:\n",
        "  - `is_leaf` (bool): To determine if the node is a leaf node.\n",
        "  - `feature_index` (int): The feature index used to split.\n",
        "  - `threshold` (float): The threshold value used to split the data.\n",
        "  - `prediction` (int): The value of the prediction (if the node is a leaf).\n",
        "  - `left` (dict): Left child node.\n",
        "  - `right` (dict): Right child node."
      ],
      "metadata": {
        "id": "nX6s0Eo0ZH_3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hmxsBreMVrvw"
      },
      "outputs": [],
      "source": [
        "def create_node(\n",
        "    is_leaf: bool,\n",
        "    feature_index: int = -1,\n",
        "    threshold: float = -1,\n",
        "    prediction: float = None,\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Creates a decision tree node.\n",
        "\n",
        "    Args:\n",
        "        is_leaf (bool): Whether the node is a leaf node.\n",
        "        feature_index (int): The feature index used for splitting.\n",
        "        threshold (float): The threshold value for splitting.\n",
        "        prediction (float): The prediction value for leaf nodes.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary representing a decision tree node.\n",
        "    \"\"\"\n",
        "    # fill code here\n",
        "\n",
        "    return {\n",
        "        \"is_leaf\": is_leaf,\n",
        "        \"feature_index\": feature_index,\n",
        "        \"threshold\": threshold,\n",
        "        \"prediction\": prediction,\n",
        "        \"left\": None,\n",
        "        \"right\": None\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Calculating Entropy [7 points]**\n",
        "\n",
        "- **Purpose**: Calculate the entropy for a given split of labels.\n",
        "-\n",
        "  - Entropy is a measure of impurity, where lower values indicate a purer split.\n",
        "  - For each subset of labels (left and right), calculate the proportion of each class.\n",
        "  - Use the formula for entropy\n",
        "  - The entropy for the split is the weighted average of the left and right entropies.\n"
      ],
      "metadata": {
        "id": "2kPYPXJsZzZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_entropy(left_labels: pd.Series, right_labels: pd.Series) -> float:\n",
        "    \"\"\"\n",
        "    Calculates the entropy for a split.\n",
        "\n",
        "    Args:\n",
        "        left_labels (pd.Series): The label values for the left split.\n",
        "        right_labels (pd.Series): The label values for the right split.\n",
        "\n",
        "    Returns:\n",
        "        float: The entropy value for the split.\n",
        "    \"\"\"\n",
        "    left_size = len(left_labels)\n",
        "    right_size = len(right_labels)\n",
        "    total_size = left_size + right_size\n",
        "\n",
        "    if total_size == 0:\n",
        "        return 0\n",
        "\n",
        "    def entropy(labels):\n",
        "\n",
        "        # fill code here\n",
        "        counts = labels.value_counts()\n",
        "        probabilities = counts / len(labels)\n",
        "        return -sum(prob * np.log2(prob) for prob in probabilities if prob > 0)\n",
        "\n",
        "\n",
        "    # Calculate entropy for both left and right splits\n",
        "    entropy_left = entropy(left_labels)\n",
        "    entropy_right = entropy(right_labels)\n",
        "\n",
        "    # Weighted average of both entropies\n",
        "    # fill code here\n",
        "    weighted_entropy = (left_size / total_size) * entropy_left + (right_size / total_size) * entropy_right\n",
        "\n",
        "    return weighted_entropy"
      ],
      "metadata": {
        "id": "BmMedLosXAzy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Splitting Dataset [4 points]**\n",
        "\n",
        "- **Purpose**: Split the dataset into left and right parts based on a given feature and threshold.\n",
        "-\n",
        "  - Iterate through each instance in the dataset.\n",
        "  - Compare the value of the selected feature with the given threshold.\n",
        "  - Append the instance to either the left or right split based on whether the feature value is less than or equal to the threshold.\n",
        "  - Make sure both the features and labels are split according"
      ],
      "metadata": {
        "id": "qWwt1ez3ZJyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(\n",
        "    df: pd.DataFrame, feature_index: str, threshold: float\n",
        ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Splits the dataset based on a feature and a threshold.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The dataset including features and labels.\n",
        "        feature_index (str): The feature to split on.\n",
        "        threshold (float): The threshold value to use for the split.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the left and right datasets.\n",
        "    \"\"\"\n",
        "    #fill code here\n",
        "    left_df = df[df[feature_index] <= threshold]\n",
        "    right_df = df[df[feature_index] > threshold]\n",
        "\n",
        "    return left_df, right_df"
      ],
      "metadata": {
        "id": "u4J2x56ZVxyd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Checking Purity [2 points]**\n",
        "\n",
        "- **Purpose**: Determine if all the labels are the same.\n",
        "-\n",
        "  - If all labels are identical, the node should be considered pure, and it will become a leaf node.\n",
        "  - Check if all elements are the same.\n",
        "  - This function helps determine whether further splitting is necessary."
      ],
      "metadata": {
        "id": "5iLawW9qZt0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_pure(labels: pd.Series) -> bool:\n",
        "    \"\"\"\n",
        "    Checks if all the labels are the same.\n",
        "\n",
        "    Args:\n",
        "        labels (pd.Series): The label values.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if all labels are the same, False otherwise.\n",
        "    \"\"\"\n",
        "    # fill code here\n",
        "    return labels.nunique() == 1"
      ],
      "metadata": {
        "id": "UzNmFLqpWCET"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Finding the Best Split [7 points]**\n",
        "\n",
        "- **Purpose**: Find the best feature and threshold to split the dataset.\n",
        "-\n",
        "  - Iterate through each feature and determine possible thresholds.\n",
        "  - Use entropy as the metric to evaluate the quality of the split.\n",
        "  - For each threshold, calculate the entropy of the resulting splits and choose the one with the lowest entropy.\n",
        "  - The best split will minimize the entropy, meaning the resulting splits are as pure as possible.\n"
      ],
      "metadata": {
        "id": "NdVhZnuqeA8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_best_split(\n",
        "    df: pd.DataFrame, features: list[str], label_column: str\n",
        ") -> tuple[str, float]:\n",
        "    \"\"\"\n",
        "    Finds the best feature and threshold to split the dataset by minimizing entropy.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The dataset including features and labels.\n",
        "        features (list[str]): The list of feature column names.\n",
        "        label_column (str): The label column name.\n",
        "\n",
        "    Returns:\n",
        "        tuple: The feature name and threshold value for the best split.\n",
        "    \"\"\"\n",
        "    best_entropy = float(\"inf\")\n",
        "    best_feature = \"\"\n",
        "    best_threshold = -1\n",
        "\n",
        "    # fill code here\n",
        "    for feature in features:\n",
        "        values = df[feature].unique()\n",
        "        for threshold in values:\n",
        "            left_df, right_df = split_dataset(df, feature, threshold)\n",
        "            left_labels = left_df[label_column]\n",
        "            right_labels = right_df[label_column]\n",
        "\n",
        "            entropy = calculate_entropy(left_labels, right_labels)\n",
        "\n",
        "            if entropy < best_entropy:\n",
        "                best_entropy = entropy\n",
        "                best_feature = feature\n",
        "                best_threshold = threshold\n",
        "\n",
        "    return best_feature, best_threshold"
      ],
      "metadata": {
        "id": "GiS-ejjLeFfr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Calculating Prediction [2 points]**\n",
        "\n",
        "- **Purpose**: Calculate the prediction value for a leaf node (most common label).\n",
        "- **Description**:\n",
        "  - This function returns the most common label in the dataset, which is used as the prediction for a leaf node.\n"
      ],
      "metadata": {
        "id": "wjpKjWbqgiU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_prediction(labels: pd.Series) -> int:\n",
        "    \"\"\"\n",
        "    Calculates the prediction value for a leaf node.\n",
        "\n",
        "    Args:\n",
        "        labels (pd.Series): The label values.\n",
        "\n",
        "    Returns:\n",
        "        int: The most common value of the labels.\n",
        "    \"\"\"\n",
        "    # fill code here\n",
        "    return labels.mode()[0]"
      ],
      "metadata": {
        "id": "FmthfOXMggx6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Building the Decision Tree [7 points]**\n",
        "\n",
        "- **Purpose**: Recursively build the decision tree by finding the best split and creating nodes.\n",
        "- **Hints**:\n",
        "  - Start with the current depth as 0 and increase it with each recursive call.\n",
        "  - Stop the recursion if the current depth reaches the maximum depth or if the node is pure.\n",
        "  - Use the `find_best_split` function to determine the best feature and threshold for splitting.\n",
        "  - Create left and right child nodes recursively and attach them to the current node."
      ],
      "metadata": {
        "id": "Ipjqbv6OaGiw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_tree(\n",
        "    df: pd.DataFrame,\n",
        "    features: list[str],\n",
        "    label_column: str,\n",
        "    current_depth: int,\n",
        "    max_depth: int,\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Builds the decision tree recursively.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The dataset including features and labels.\n",
        "        features (list[str]): The list of feature column names.\n",
        "        label_column (str): The label column name.\n",
        "        current_depth (int): The current depth of the tree.\n",
        "        max_depth (int): The maximum allowed depth of the tree.\n",
        "\n",
        "    Returns:\n",
        "        dict: The root node of the decision tree.\n",
        "    \"\"\"\n",
        "    # fill code for base case\n",
        "    if current_depth >= max_depth or is_pure(df[label_column]):\n",
        "        prediction = calculate_prediction(df[label_column])\n",
        "        return create_node(is_leaf=True, prediction=prediction)\n",
        "\n",
        "\n",
        "    # fill code to find best feature\n",
        "    best_feature, best_threshold = find_best_split(df, features, label_column)\n",
        "\n",
        "    # fill code to split the data\n",
        "    left_split, right_split = split_dataset(df, best_feature, best_threshold)\n",
        "\n",
        "    # recursively building left and right subtrees\n",
        "    left_node = build_tree(left_split, features, label_column, current_depth + 1, max_depth)\n",
        "    right_node = build_tree(right_split, features, label_column, current_depth + 1, max_depth)\n",
        "\n",
        "    # Create current internal node\n",
        "    node = create_node(\n",
        "        is_leaf=False,\n",
        "        feature_index=best_feature,\n",
        "        threshold=best_threshold\n",
        "    )\n",
        "    node[\"left\"] = left_node\n",
        "    node[\"right\"] = right_node\n",
        "\n",
        "\n",
        "    return node"
      ],
      "metadata": {
        "id": "SBbCBDttWEfV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Predicting Using the Tree [3 points]**\n",
        "\n",
        "- **Purpose**: Predict the label for a given feature vector using the trained decision tree.\n",
        "- **Hints**:\n",
        "  - Start at the root of the tree and traverse down to a leaf node.\n",
        "  - For each internal node, decide whether to move left or right based on the feature value and threshold.\n",
        "  - When a leaf node is reached, return its prediction value."
      ],
      "metadata": {
        "id": "LTc14KM3Z9d-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(tree: dict, sample: pd.Series) -> int:\n",
        "    \"\"\"\n",
        "    Predicts the label for a given feature vector using the decision tree.\n",
        "\n",
        "    Args:\n",
        "        tree (dict): The root node of the decision tree.\n",
        "        sample (pd.Series): The feature vector.\n",
        "\n",
        "    Returns:\n",
        "        int: The predicted label.\n",
        "    \"\"\"\n",
        "    # fill code here\n",
        "    node = tree\n",
        "\n",
        "    # Traverse until reaching a leaf node\n",
        "    while not node[\"is_leaf\"]:\n",
        "        feature_val = sample[node[\"feature_index\"]]\n",
        "        threshold = node[\"threshold\"]\n",
        "\n",
        "        if feature_val <= threshold:\n",
        "            node = node[\"left\"]\n",
        "        else:\n",
        "            node = node[\"right\"]\n",
        "\n",
        "    return node[\"prediction\"]"
      ],
      "metadata": {
        "id": "cKB37Y4yYYg7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using a Real-World Dataset and Comparison with Scikit-Learn"
      ],
      "metadata": {
        "id": "2Iv-KI50a6GJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Iris dataset\n",
        "iris = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\", header=None)\n",
        "iris.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']"
      ],
      "metadata": {
        "id": "ETeO8A8iXPAR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare features and labels\n",
        "features = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
        "label_column = 'class'\n",
        "iris['class'] = iris['class'].apply(lambda x: 0 if x == 'Iris-setosa' else (1 if x == 'Iris-versicolor' else 2))"
      ],
      "metadata": {
        "id": "neBVD1-SgG_n"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and testing\n",
        "X_train, X_test = train_test_split(iris, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "blmX6OqiXL_U"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_depth = 3\n",
        "decision_tree = build_tree(X_train, features, label_column, 0, max_depth)"
      ],
      "metadata": {
        "id": "NvLtKsgwXNiu"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = X_test.apply(lambda row: predict(decision_tree, row), axis=1)"
      ],
      "metadata": {
        "id": "Ya0vV8bVgwyb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_accuracy = accuracy_score(X_test[label_column], predictions)\n",
        "print(f\"Custom Decision Tree Accuracy: {custom_accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TO5Rlqcgytr",
        "outputId": "c19de647-336c-46f8-8ac7-201344f4ef7c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom Decision Tree Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf = DecisionTreeClassifier(max_depth=max_depth, random_state=42)\n",
        "clf.fit(X_train[features], X_train[label_column])\n",
        "sklearn_predictions = clf.predict(X_test[features])\n",
        "sklearn_accuracy = accuracy_score(X_test[label_column], sklearn_predictions)\n",
        "print(f\"Scikit-Learn Decision Tree Accuracy: {sklearn_accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebSrnFBQXe_f",
        "outputId": "9f95dbb9-740d-46d2-c061-183d4a425493"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scikit-Learn Decision Tree Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forests (BONUS) [10 points]\n",
        "\n",
        "Using the algorithm discussed in class, implement the random forest algorithm. You must reuse some of the Decision Trees functions you wrote above. You must call and use the random forest algorithm you build on the IRIS dataset."
      ],
      "metadata": {
        "id": "17HpRMvGirTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bootstrap_sample(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    # creating a bootstrap sample of input dataframe\n",
        "    sample = df.sample(n=len(df), replace=True)\n",
        "    return sample"
      ],
      "metadata": {
        "id": "9cUMKcYXJH-A"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def build_random_forest(df: pd.DataFrame, features: list, label_column: str, n_trees: int, max_depth: int,\n",
        "                  max_features_ratio: float = 0.5) -> dict:\n",
        "    forest = {}\n",
        "\n",
        "    for i in range(n_trees):\n",
        "        bootstrapped_df = bootstrap_sample(df)\n",
        "\n",
        "        # Randomly select a subset of features\n",
        "        n_features = int(len(features) * max_features_ratio)\n",
        "        sampled_features = random.sample(features, n_features)\n",
        "\n",
        "        tree = build_tree(bootstrapped_df, sampled_features, label_column, 0, max_depth)\n",
        "        forest[f\"tree_{i}\"] = (tree, sampled_features)\n",
        "\n",
        "    return forest\n"
      ],
      "metadata": {
        "id": "zChysSvz0g9E"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forest_prediction(forest: dict, sample: pd.Series) -> int:\n",
        "    from collections import Counter\n",
        "    predictions = []\n",
        "\n",
        "    for tree_key in forest:\n",
        "        tree, tree_features = forest[tree_key]\n",
        "        prediction = predict(tree, sample[tree_features])\n",
        "        predictions.append(prediction)\n",
        "\n",
        "    return Counter(predictions).most_common(1)[0][0]\n"
      ],
      "metadata": {
        "id": "Q0uG4VzM1gav"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Iris dataset\n",
        "iris = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\", header=None)\n",
        "iris.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']"
      ],
      "metadata": {
        "id": "65GM2Jvf3MMW"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare features and labels\n",
        "features = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
        "label_column = 'class'\n",
        "iris['class'] = iris['class'].apply(lambda x: 0 if x == 'Iris-setosa' else (1 if x == 'Iris-versicolor' else 2))"
      ],
      "metadata": {
        "id": "aXUvQOqt3Qof"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(iris[features], iris[label_column], test_size=0.2, random_state=42, shuffle=True)"
      ],
      "metadata": {
        "id": "qG6x6PceJnJL"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preparing training data\n",
        "train_df = X_train.copy()\n",
        "train_df[label_column] = y_train\n",
        "\n",
        "# training forest\n",
        "n_trees = 10\n",
        "max_depth = 3\n",
        "forest = build_random_forest(train_df, features, label_column, n_trees, max_depth)\n",
        "\n",
        "# calculating accuracy\n",
        "rf_predictions = 0\n",
        "for i in range(len(X_test)):\n",
        "    test_sample = X_test.iloc[i]\n",
        "    prediction = forest_prediction(forest, test_sample)\n",
        "    if prediction == y_test.iloc[i]:\n",
        "        rf_predictions += 1\n",
        "\n",
        "rf_accuracy = rf_predictions / len(X_test)\n",
        "print(f\"Accuracy: {rf_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_CXWlyjusEc",
        "outputId": "750341bb-448d-40ae-f2a1-2bb346a94003"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9666666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression [15 points]"
      ],
      "metadata": {
        "id": "sKFhGL2sh9TN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Show that: [5 points]\n",
        "\n",
        "$$\n",
        "\\sigma(-x) = 1 - \\sigma(x)\n",
        "$$\n",
        "\n",
        "where\n",
        "$$\n",
        "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
        "$$"
      ],
      "metadata": {
        "id": "2npBupW1ih72"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "\n",
        "<br>\n",
        "\n",
        "Given the definition of $\\sigma(x)$, we know that:\n",
        "\n",
        "$$\\sigma(-x) = \\frac{1}{1 + e^{-(-x)}} = \\frac{1}{1 + e^x}$$\n",
        "\n",
        "and\n",
        "\n",
        "$$1 - \\sigma(x) = 1 - \\frac{1}{1 + e^{-x}} = \\frac{(1 + e^{-x}) - 1}{1 + e^{-x}} = \\frac{e^{-x}}{1 + e^{-x}} = \\frac{1}{1 + e^x}$$\n",
        "\n",
        "<br>\n",
        "\n",
        "Since $\\sigma(-x)$ and $ 1-\\sigma(x) $ are both equal to $\\frac{1}{1 + e^x}$, this proves that $\\sigma(-x) = 1 - \\sigma(x)$.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gyrpL9WjAjPl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Show that: [5 points]\n",
        "\n",
        "$$\n",
        "\\frac{d}{dx}\\sigma(x) = \\sigma(x)(1 - \\sigma(x))\n",
        "$$\n"
      ],
      "metadata": {
        "id": "W4JcVkiRimXF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "\n",
        "<br>\n",
        "\n",
        "From part 1, we are given that $\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n",
        "\n",
        "<br>\n",
        "\n",
        "Taking the derivative of $\\sigma(x)$, we have:\n",
        "\n",
        "$$\\frac{d}{dx} \\sigma(x) = \\frac{d}{dx} \\left( \\frac{1}{1 + e^{-x}} \\right)\n",
        "= \\frac{e^{-x}}{(1 + e^{-x})^2}$$\n",
        "\n",
        "<br>\n",
        "\n",
        "From part 1, we also know that $1 - \\sigma(x) = \\frac{e^{-x}}{1 + e^{-x}}$\n",
        "\n",
        "<br>\n",
        "\n",
        "If we multiply $\\sigma(x)$ and $1-\\sigma(x)$, we have:\n",
        "\n",
        "$$\n",
        "\\sigma(x)(1 - \\sigma(x)) = \\left( \\frac{1}{1 + e^{-x}} \\right) \\left( \\frac{e^{-x}}{1 + e^{-x}} \\right) = \\frac{e^{-x}}{(1 + e^{-x})^2}\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "Since the product of $\\sigma(x)(1 - \\sigma(x))$ is equivalent to the derivative of $\\sigma(x)$, we have therefore shown that $\\frac{d}{dx} \\sigma(x) = \\sigma(x)(1 - \\sigma(x))$."
      ],
      "metadata": {
        "id": "bNaHU7X-BlDG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Gradient Descent [5 points]\n",
        "You have a univariate function you wish to minimize, $f(w) = 5(w-11)^4$. Suppose you wish to perform gradient descent with constant step size $\\alpha = 1/40$. Starting with $w_0 = 13$, perform 5 steps of gradient descent. What are $w_0, w_1, w_2, w_3, w_4, w_5$? What is the value of $f(w_5)$?"
      ],
      "metadata": {
        "id": "J-BFtOYtjJSk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "\n",
        "<br>\n",
        "\n",
        "Given $f(w) = 5(w - 11)^4$, we compute the gradient of $f(w)$ using the chain rule:\n",
        "\n",
        "$$f'(w) = \\frac{d}{dw} \\left[ 5(w - 11)^4 \\right] = 20(w - 11)^3$$\n",
        "\n",
        "<br>\n",
        "\n",
        "Therefore the gradient descent update rule is:\n",
        "\n",
        "$$w_{t+1} = w_t - \\alpha \\cdot f'(w_t) = w_t - \\frac{1}{40} \\cdot 20(w_t - 11)^3 = w_t - \\frac{1}{2}(w_t - 11)^3$$\n",
        "\n",
        "<br>\n",
        "\n",
        "Starting with $w_0 = 13$, we can compute the following five steps:\n",
        "\n",
        "$$w_0 = 13$$\n",
        "\n",
        "$$w_1 = 13 - \\frac{1}{2}(13 - 11)^3 = 13 - \\frac{1}{2}(2^3) = 13 - 4 = 9$$\n",
        "\n",
        "$$w_2 = 9 - \\frac{1}{2}(9 - 11)^3 = 9 - \\frac{1}{2}(-2)^3 = 9 + 4 = 13$$\n",
        "\n",
        "$$w_3 = 13 - \\frac{1}{2}(13 - 11)^3 = 13 - 4 = 9$$\n",
        "\n",
        "$$w_4 = 9 - \\frac{1}{2}(9 - 11)^3 = 9 + 4 = 13$$\n",
        "\n",
        "$$w_5 = 13 - \\frac{1}{2}(13 - 11)^3 = 13 - 4 = 9$$\n",
        "\n",
        "<br>\n",
        "\n",
        "Therefore, the value of $f(w_5)$ is:\n",
        "\n",
        "$$w_5 = 9$$  \n",
        "$$f(w_5) = 5(9 - 11)^4 = 5(-2)^4 = 5 \\cdot 16 = 80$$\n",
        "\n",
        "<br>\n",
        "\n",
        "Thus:\n",
        "$$f(w_5) = 80$$\n"
      ],
      "metadata": {
        "id": "99griSySBnyq"
      }
    }
  ]
}